% Homework LaTeX Template
% original version by Sayan Chaudhry (sayanc)
% modified by Tan Yan (tanyan)

\documentclass[11pt]{article}

\title{PageRank}
\author{Tan Yan \and Daniel Bae}

% Useful Packages
\usepackage{amsmath}  % Packages to make typesetting math
\usepackage{amssymb}  %   stuff and symbols easy by the
\usepackage{amsthm}   %   American Mathematical Society
\usepackage{textcomp} % Package for some special symbols
\usepackage{fancyhdr} % Package to make pretty headers
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{hyperref} % Package to add image support
\hypersetup{          
    colorlinks=true,  
    linkcolor=red,    
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{graphicx} % Package to add image support
\graphicspath{        
    {assets/}         % Upload all images in assets/
}

% Theorem Styles
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{notation}[theorem]{Notation}

% Page Setup
\oddsidemargin0cm
\topmargin-2cm
\textwidth16.5cm
\textheight23.5cm
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headsep}{1.7em}
\setlength{\headheight}{30pt}
\renewcommand{\baselinestretch}{1.25}

% Math Commands :: General
\newcommand{\micdrop}{\hfill \qedsymbol}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\Lap}[1]{\mathcal{L}\left\{#1\right\}}

% Math Commands :: Proofs 
\newcommand{\wts}{\text{We want to show that }}
\newcommand{\st}{\text{ such that }}
\newcommand{\afsoc}{\text{Assume for the sake of contradiction that }}
\newcommand{\Wlog}{\text{Without loss of generality, assume that }}

% Math Commands :: Sets
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\emptyset}{\varnothing}

% Math Commands :: Induction
\newcommand{\rhs}{\texttt{RHS}}
\newcommand{\lhs}{\texttt{LHS}}
\newcommand{\bc}{\texttt{Base Case}}
\newcommand{\bcs}{\texttt{Base Cases}}
\newcommand{\ih}{\texttt{Induction Hypothesis}}
\newcommand{\is}{\texttt{Induction Step}}

% Math Commands :: Probability
\newcommand{\p}[1]{\mathbb{P}\left\{#1\right\}}
\newcommand{\e}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\mathrm{Var}\left[#1\right]}

% Math Commands :: Linear Algebra
\DeclareMathOperator{\Diam}{diam}
\newcommand{\diam}[1]{\Diam\left(#1\right)}
\DeclareMathOperator{\nul}{Null}
\newcommand{\Null}[1]{\nul\left(#1\right)}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\threevec}[3]{\left[\begin{array}{r} #1 \\ #2 \\ #3\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r} #1 \\ #2 \\ #3\\#4\end{array}\right]}
\DeclareMathOperator{\Rank}{rank}
\newcommand{\rank}[1]{\Rank\left(#1\right)}
\DeclareMathOperator{\Sp}{Span}
\renewcommand{\sp}[1]{\Sp\left\{#1\right\}}
\DeclareMathOperator{\col}{Col}
\newcommand{\Col}[1]{\col\left(#1\right)}

\begin{document}
\maketitle

\section{Introduction}
The early search engines of the Internet used text-based ranking systems that assigned relevance to webpages based on the highest number of keywords present. 
Although this approach intuitively makes sense, it often ranked webpages with a high volume of keywords and no other content as relevant results to a search, which is not desired behavior.
A better approach to generating webpage rankings is to examine the links to that page.
Under this schema, a page is highly ranked if it has many links or a highly-ranked page directed to it, encapsulating both popularity and authority into a ranking.

\section{Definitions}
\begin{definition}
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a countable probability space, where
    \begin{enumerate}
        \item The outcome space $\Omega$ is some countable set of webpages on the internet;
        \item The set of all possible events $\mathcal{F} = \{ \{\omega\} \mid \omega \in \Omega \}$ is set of all events consisting of exactly one outcome, i.e. the event that a web surfer is on a certain webpage;
        \item The probability measure $\mathbb{P}$ is a function $\mathcal{F} \to [0,1]$ denoting the probability of the event that a web surfer is on a certain webpage.
    \end{enumerate}
\end{definition}

\begin{definition}
    Let a random walk of some countable set of webpages $\Omega$ be a stochastic process on $(\Omega, \mathcal{F}, \mathbb{P})$ defined as follows:

    Let $\Omega$ be enumerated by the set $S = \{0, 1, 2, 3, \ldots\} \subseteq \N$, such that $\Omega = \{\omega_i \mid i \in S\}$.
    We call $S$ the state space of the stochastic process, and note that $S$ is countable.

    Let the index set $T = \{0, 1, 2, 3, \ldots \} = \N$ denote the number of links that the web surfer has clicked on, i.e. the number of times that the web surfer has jumped from one webpage to another.
    As $T$ represents the measure of time in the process and $T$ is countable, we say that the stochastic process occurs in discrete time.
    
    Let $\{X_t : \Omega \to S\}_{t \in T}$ be a family of $S$-valued random variables indexed by $T$ such that
    $\p{X_t = i}$ corresponds to the probability of the event that the web surfer lands on the $i$-th webpage after clicking on $t$ links or making $t$ jumps from one webpage to another.

    Then $\{X_t : \Omega \to S\}_{t \in T}$ precisely describes a discrete-time stochastic process over a countable state space on $(\Omega, \mathcal{F}, \mathbb{P})$.
    Furthermore, we assume that a random walk of $\Omega$ satisfies the Markov property.
\end{definition}

\begin{definition}
    (The Markov property) Let a stochastic process on some countable probability space $(\Omega, \mathcal{F}, \mathbb{P})$
    with countable state space $S$ be described by $\{X_t : \Omega \to S\}_{t \in T}$,
    where the index set $T$ is exactly $\{0, 1, 2, 3, \ldots \} = \N$,
    i.e. it occurs in discrete time.

    Then this stochastic process is said to be memoryless, a.k.a. possesses the Markov property, a.k.a. is a discrete-time Markov chain, if and only if for all $t \in T$ and $x_0, x_1, \ldots, x_t \in S$:
    $$\p{X_t = x_t \mid X_{t-1} = x_{t-1}, \ldots, X_0 = x_0} = \p{X_t = x_t \mid X_{t-1} = x_{t-1}}$$
    which implies
    $$\p{X_t = x_t} = \sum_{x_{t-1} \in S} \p{X_t = x_t \mid X_{t-1} = x_{t-1}} \p{X_{t-1} = x_{t-1}}$$
    For a random walk of some countable set of webpages $\Omega$, this property implies that 
    the probability that the web surfer lands on a certain webpage after clicking on $t$ links is solely dependent on
    which webpage the web surfer was on before clicking the $t$-th link.
\end{definition}

\begin{definition}
    Let a random walk of some finite set of $n$ webpages $\Omega$ be a discrete-time Markov chain on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$
    be described by $\{X_t : \Omega \to S\}_{t \in T}$, where $S = \{1, 2, \ldots, n\} = [n]$ and $T = \{0, 1, 2, 3, \ldots\} = \N$.
    
    Then, for each $t \in T$, let $\vec{v}_t$ be a $n \times 1$ vector
    % $\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$,
    where $v_i = \p{X_t = i}$ for $i \in S$.
    Observe that $\vec{v}_t$ corresponds to the probability distribution of which webpage the web surfer is on after clicking $t$ links.
    We say that $\vec{v}_t$ is a probablistic vector describing the state at step $t$.

    Furthermore, let $P$ be a $n \times n$ matrix
    %$\begin{bmatrix}
    %    p_{11} & \cdots & p_{1n} \\
    %    \vdots & \ddots & \vdots \\
    %    p_{n1} & \cdots & p_{nn} \\
    %\end{bmatrix}$,
    where $P_{ij} = \p{X_t = i \mid X_{t-1} = j}$ for $i, j \in S$ and any arbitrary $t \in T$. Observe that by Definition 3, $\vec{v}_t = P \vec{v}_{t-1}$ for all $t \in T$.
    So multiplying by $P$ corresponds to the web surfer clicking a link.
    Thus we say that $P$ is the unique Markov transition matrix that corresponds to the given discrete-time Markov chain.
\end{definition}

\begin{definition}
    A matrix is said to be column-stochastic if and only if each of its entries are between 0 and 1 inclusive, and each of its columns sum up to 1.
    Similarly, a vector is said to be a stochastic or probablistic vector if and only if its components are between 0 and 1 inclusive and sum up to 1.
    Note that the matrix-vector product between a column-stochastic matrix a probabilistic vector always yields a probabilistic vector.
\end{definition}

\begin{definition}
    An Markov chain is said to be irreducible if it is possible to get from any state from any state.
    For a random walk of of webpages, this property implies that it is possible to get to any webpage from any any webpage by click some series of links.
    Note that a Markov transition matrix with all positive entries necessarily corresponds to an irreducible Markov chain.
\end{definition}

\section{Main Ideas}

% initialize initial state v_0 to ... because ...
% observe that for some random walk defined by some Markov transition matrix P, the state at the n-th step is P^n v_0
% so after inf num of steps, prob web surf on page is limit ... which represent relevance of webpages

We will now rank a set of $n$ webpages by simulating a random walk of the webpages.
We initialize the initial state vector $\vec{v}_0$ with all of its components equal to $\frac{1}{n}$,
representing the assumption that a web surfer is equally likely to be initially on any of the $n$ webpages.
Now observe that for a random walk defined by some Markov transition matrix $P$, the state at the $t$-th step,
or the probability distribution of which webpage the web surfer is on after clicking $t$ links, is exactly $\vec{v}_t = P^t \vec{v}_0$.

We note that the relevance of a given webpage can be represented by the probability that a web surfer is on the webpage after some arbitrarily large number of steps of a random walk.
% the probability that the web surfer is on a given page is proportional to the number of in-bound links to that page.
Therefore, we want to find the probabilistic vector $\vec{v}^* = \lim\limits_{t\to \infty}{P^t \vec{v}_0}$,
which is the stationary distribution of the random walk corresponding to the Markov transition matrix $P$.
Since we can sort the components of $\vec{v}^*$ to rank the webpages by their relevance, we appropriately call $\vec{v}^*$ the PageRank vector.

% to find this limit we can cite the following theorems without proof

In order to find the PageRank vector, we cite without proof the following theorem.
\begin{theorem}
    (Perron-Frobenius Theorem) If $M$ is a positive column-stochastic matrix, then
    \begin{enumerate}[i.]
        \item 1 is an eigenvalue of multiplicity one.
        \item 1 is the largest eigenvalue. All other eigenvalues have absolute value less than 1.
        \item The eigenvectors corresponding to the eigenvalue 1 have either only positive entries or only negative entries.
        As such, there exists a unique probabilistic vector that is also an eigenvector corresponding to the the eigenvalue 1.
    \end{enumerate}
    % $\lambda_1, \lambda_2, \cdots, \lambda_n$ be the eigenvalues of $M$ sorted in decreasing order of magnitude. Then 1 is an eigenvalue with multiplicity 1 such that $\lambda_1=1$ and there exists a corresponding probabilistic eigenvector.
\end{theorem}

%Since the PageRank matrix $M$ is a postitive column-stochastic matrix by construction, by Perron-Frobenius theorem we know that there exists a probabilistic eigenvector $\vec{v}^*$ corresponding to the eigenvalue 1.

% In order to show that the PageRank vector is equal to $\vec{v}^*$, we apply the next theorem that exploits the fact that all of the eigenvectors $\lambda_i$ for $1<i\leq n$ of $M$ are less than $1$.
\begin{theorem}
    Let $M$ be a $n\times n$ positive column-stochastic matrix, with $\vec{v}^*$ denoting the probabilstic vector corresponding to the eigenvalue 1.
    Then for any initial state with probabilistic vector $\vec{v}_0$, the limit $\lim\limits_{t\to \infty}{M^t \vec{v}_0}$ converges to $\vec{v}^*$.    
\end{theorem}
\begin{proof}
    Let $\lambda_1, \lambda_2, \ldots, \lambda_k$ be the eigenvalues of $M$, where $k \leq n$, and $\vec{w}_1, \vec{w}_2, \ldots, \vec{w}_k$ be their corresponding eigenvectors, 
    such that $M \vec{w}_i = \lambda_i \vec{w}_i$ for all $i \in [k]$.
    We can express $\vec{v}_0$ as a linear combination of the eigenvectors of $M$, so
    $$\vec{v}_0 = \alpha_1 \vec{w}_1 + \alpha_2 \vec{w}_2 + \cdots + \alpha_k \vec{w}_k$$
    for some scalars $\alpha_1, \alpha_2, \ldots, \alpha_k$. Multiplying by $M^t$ on both sides yields
    $$M^t \vec{v}_0 = \alpha_1 M^t \vec{w}_1 + \alpha_2 M^t \vec{w}_2 + \cdots + \alpha_k M^t \vec{w}_k$$
    $$M^t \vec{v}_0 = \alpha_1 \lambda_1^t \vec{w}_1 + \alpha_2 \lambda_2^t \vec{w}_2 + \cdots + \alpha_k \lambda_k^t \vec{w}_k$$
    By the Perron-Frobenius Theorem, we can let $\lambda_1 = 1$,
    and for all $1 < i \leq k$, since $\abs{\lambda_i} < 1$, $\lim\limits_{t\to \infty}{\lambda_i^t} = 0$.
    Thus, taking the limit yields $\lim\limits_{t\to \infty}{M^t \vec{v}_0} = \alpha_1 \vec{w}_1$, which is an eigenvector corresponding to the eigenvalue 1.
    But recall that $M^t \vec{v}_0 = \vec{v}_t$ must be a probabilistic vector,
    and since $\vec{v}^*$ is the unique probabilstic vector corresponding to the eigenvalue 1, $\lim\limits_{t\to \infty}{M^t \vec{v}_0} = \vec{v}^*$. as desired.
\end{proof}

Thus, for some random walk corresponding to the Markov transition matrix $M$, 
the PageRank vector is equal to the probabilistic eigenvector of $M$ corresponding to the eigenvalue of 1, but this requires $M$ to be a positive column-stochastic matrix!
We shall now examine two special cases that illuminate why $M$ must be positive and column-stochastic.

\begin{definition}
    Let $A$ be an adjacency matrix that represents a directed graph consisted of some finite set of $n$ webpages enumerated 1 to $n$,
    with the $k$-th webpage represented as node $k$, and links between two webpages as edges,
    such that $A_{ij} = 1$ if and only if there exists an edge from node $j$ to node $i$, i.e. the $j$-th webpage has a link to the $i$-th webpage, and $A_{ij} = 0$ otherwise.
\end{definition}

\begin{definition}
    Let $L(j)$ denote the number of outgoing links from the $j$-th webpage.
    Given a $n \times n$ adjacency matrix $A$ representing a directed graph of $n$ webpages, 
    we have $L(j) = \sum_{i = 1}^n A_{ij}$.
    If there exists a node $k$ such that $L(k) = 0$, i.e. the $k$-th webpage has no outgoing links to other webpages, we say that node $k$ is a dangling node.
\end{definition}

% why we can't have dangling node: not column-stochastic!

Consider a graph of webpages that contains a dangling node.
This means that the matrix $A$ would contain a column full of zeros at the dangling node.
If we naively define a Markov transition matrix $P$ such that the web surfer can only follow the links from one webpage to another,
then when the web surfer reaches a dangling node, i.e. a webpage with no outgoing links, the web surfer will have zero probability of reaching any other webpage,
thus resulting in a column full of zeros in $P$.
As such, $P$ will not be column-stochastic, or even a well-defined Markov transition matrix, and in fact,
$P^t \vec{v}_0$ would converge to $\vec{0}$, regardless of the other links recorded in $P$.


% how we fix this

In order to prevent this first type of pathology from occuring, we define the Markov transition matrix $P$ for a random walk of $n$ webpages such that
a web surfer has equal probability to follow any of the links present on a webpage,
but upon reaching a dangling node, the web surfer has equal probability of transitioning to any random node, analogous to how a web surfer can type a hyperlink directly to a webpage.

\begin{definition}
    Let the Markov transition matrix $P$ of a random walk of some finite set of $n$ webpages be constructed as follows:
    given a $n \times n$ adjacency matrix $A$ representing a directed graph of the $n$ webpages,
    for each entry $p_{ij}$ in $P$, with $i,j \in [n]$,
    $$ P_{ij} = \begin{cases}
        \dfrac{1}{L(j)} & \text{ if } A_{ij} = 1 \\
        \dfrac{1}{n} & \text{ if } A_{ij} = 0 \text{ and } L(j) = 0 \\
        0 & \text{ otherwise }
    \end{cases}$$
\end{definition}

Below is the Python code used to construct $P$ from an adjacency list representation of the graph of webpages.

\lstinputlisting[language=Python, firstline=3, lastline=12]{pagerank.py}

% observe that now P is column stochastic by construction
% but P not necessarily positive!
% so the Markov chain that correpsonds to P may not be irreducible
% why this is bad
% how we fix this 

Note that now, $P$ is column-stochastic by construction. However, $P$ is not necessarily a matrix with all positive entires.
i.e. $P$ correpsonds to a Markov chain that is not necessarily irreducible.

A random walk of webpages that is a reducible Markov chain runs into the issue that a web surfer can potentially become trapped in a cycle of webpages,
where there is no links to any webpages outside the cycle.
In this case, after some arbitrarily large number of steps in the random walk,
the web surfer will have zero probability of reaching any webpages outside of the cycle.
Then the PageRank vector will output zero relevance for any webpages outside of the cycle, even if they have inbound links, which is incorrect. 

In order to handle this pathology, we again consider how a web surfer can type a hyperlink directly to a webpage.
By defining some nonzero probability of the web surfer ``jumping'' to a random webpage, even if there are no links to that webpage on the current page,
we allow the web surfer to get from any webpage to any webpage, thus ensuring that the random walk is a irreducible Markov chain.
This yields the PageRank matrix $M$, which uses weighted sum of the web surfer following a link and randomly ``jumping'' to create a positive column-stochastic Markov transition matrix.

\begin{definition}
    Let $M$ denote the ``PageRank Matrix'' or ``Google Matrix'' devised by Page and Brin:
    $$M = (1-d)P + \frac{d}{n} \vec{1}_{n \times n}$$
    where $P$ is the Markov transition matrix of a random walk of some finite set of $n$ webpages as defined above,
    $\vec{1}_{n \times n}$ denotes the $n \times n$ matrix consisting of entries all equal to 1,
    and $d$ denotes the ``damping factor'', representing the probability of a web surfer randomly traveling from one page to any other page,
    such that $0 < d < 1$.
\end{definition}

% now we good!
Since $M$ is positive and column-stochastic by construction, we are able to use the theorems above to find the PageRank vector.

Below is the Python code to create the PageRank matrix, find the PageRank vector, and sort the PageRank vector to rank the webpages by relevance.
Note that the variable \texttt{d} used in the below code snippet is equal to $1-d$, where $d$ is the damping factor as defined above.
This simply means that \texttt{d} represents the probability of the web surfer following the links on a webpage instead of randomly jumping.
As per Page and Brin, \texttt{d} is defined to have a default value of $0.85$.

\lstinputlisting[language=Python, firstline=13]{pagerank.py}

% \section{Results}

\section{Conclusion}
% what we wanted to accomplish
% how we did it
% future possible improvements:
%  > approximating limit prob e-vect with power method (why efficient?)
%  > using sparse multiplication (why is M sparse?)



\begin{thebibliography}{9}
\bibitem{}

\end{thebibliography}
\end{document}


