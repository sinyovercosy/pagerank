% Homework LaTeX Template
% original version by Sayan Chaudhry (sayanc)
% modified by Tan Yan (tanyan)

\documentclass[11pt]{article}

\title{PageRank}
\author{Tan Yan \and Daniel Bae}

% Useful Packages
\usepackage{amsmath}  % Packages to make typesetting math
\usepackage{amssymb}  %   stuff and symbols easy by the
\usepackage{amsthm}   %   American Mathematical Society
\usepackage{textcomp} % Package for some special symbols
\usepackage{fancyhdr} % Package to make pretty headers
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{hyperref} % Package to add image support
\hypersetup{          
    colorlinks=true,  
    linkcolor=red,    
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{graphicx} % Package to add image support
\graphicspath{        
    {assets/}         % Upload all images in assets/
}

% Theorem Styles
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{notation}[theorem]{Notation}

% Page Setup
\oddsidemargin0cm
\topmargin-2cm
\textwidth16.5cm
\textheight23.5cm
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headsep}{1.7em}
\setlength{\headheight}{30pt}
\renewcommand{\baselinestretch}{1.25}

% Math Commands :: General
\newcommand{\micdrop}{\hfill \qedsymbol}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\Lap}[1]{\mathcal{L}\left\{#1\right\}}

% Math Commands :: Proofs 
\newcommand{\wts}{\text{We want to show that }}
\newcommand{\st}{\text{ such that }}
\newcommand{\afsoc}{\text{Assume for the sake of contradiction that }}
\newcommand{\Wlog}{\text{Without loss of generality, assume that }}

% Math Commands :: Sets
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\emptyset}{\varnothing}

% Math Commands :: Induction
\newcommand{\rhs}{\texttt{RHS}}
\newcommand{\lhs}{\texttt{LHS}}
\newcommand{\bc}{\texttt{Base Case}}
\newcommand{\bcs}{\texttt{Base Cases}}
\newcommand{\ih}{\texttt{Induction Hypothesis}}
\newcommand{\is}{\texttt{Induction Step}}

% Math Commands :: Probability
\newcommand{\p}[1]{\mathbb{P}\left\{#1\right\}}
\newcommand{\e}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\mathrm{Var}\left[#1\right]}

% Math Commands :: Linear Algebra
\DeclareMathOperator{\Diam}{diam}
\newcommand{\diam}[1]{\Diam\left(#1\right)}
\DeclareMathOperator{\nul}{Null}
\newcommand{\Null}[1]{\nul\left(#1\right)}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\threevec}[3]{\left[\begin{array}{r} #1 \\ #2 \\ #3\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r} #1 \\ #2 \\ #3\\#4\end{array}\right]}
\DeclareMathOperator{\Rank}{rank}
\newcommand{\rank}[1]{\Rank\left(#1\right)}
\DeclareMathOperator{\Sp}{Span}
\renewcommand{\sp}[1]{\Sp\left\{#1\right\}}
\DeclareMathOperator{\col}{Col}
\newcommand{\Col}[1]{\col\left(#1\right)}

\begin{document}
\maketitle

\section{Introduction}
The early search engines of the Internet used text-based ranking systems that assigned relevance to webpages based on the highest number of keywords present. 
Although this approach intuitively makes sense, it often ranked webpages with a high volume of keywords and no other content as relevant results to a search, which is not desired behavior.
A better approach to generating webpage rankings is to examine the links to that page.
Under this schema, a page is highly ranked if it has many links or a highly-ranked page directed to it, encapsulating both popularity and authority into a ranking.

\section{Definitions}
\begin{definition}
    Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a countable probability space, where
    \begin{enumerate}
        \item The outcome space $\Omega$ is some countable set of webpages on the internet;
        \item The set of all possible events $\mathcal{F} = \{ \{\omega\} \mid \omega \in \Omega \}$ is set of all events consisting of exactly one outcome, i.e. the event that a web surfer is on a certain webpage;
        \item The probability measure $\mathbb{P}$ is a function $\mathcal{F} \to [0,1]$ denoting the probability of the event that a web surfer is on a certain webpage.
    \end{enumerate}
\end{definition}

\begin{definition}
    Let a random walk of some countable set of webpages $\Omega$ be a stochastic process on $(\Omega, \mathcal{F}, \mathbb{P})$ defined as follows:

    Let $\Omega$ be enumerated by the set $S = \{0, 1, 2, 3, \ldots\} \subseteq \N$, such that $\Omega = \{\omega_i \mid i \in S\}$.

    Let the index set $T = \{0, 1, 2, 3, \ldots \} = \N$ denote the number of links that the web surfer has clicked on, i.e. the number of times that the web surfer has jumped from one webpage to another.
    
    Let $\{X_t : \Omega \to S\}_{t \in T}$ be a family of $S$-valued random variables indexed by $T$ such that
    $\p{X_t = i}$ corresponds to the probability of the event that the web surfer lands on the $i$-th webpage after clicking on $t$ links or making $t$ jumps from one webpage to another.

    Then $\{X_t : \Omega \to S\}_{t \in T}$ precisely describes a stochastic process on $(\Omega, \mathcal{F}, \mathbb{P})$.
    Furthermore, we assume that a random walk of $\Omega$ satisfies the Markov property.
\end{definition}

\begin{definition}
    (The Markov property) Let a stochastic process on some countable probability space $(\Omega, \mathcal{F}, \mathbb{P})$
    with countable state space $S$ be described by $\{X_t : \Omega \to S\}_{t \in T}$,
    where the index set $T$ is exactly $\{0, 1, 2, 3, \ldots \} = \N$.
    
    Then this stochastic process is said to be memoryless, a.k.a. possesses the Markov property, a.k.a. is a Markov process, if and only if for all $t \in T$ and $x_0, x_1, \ldots, x_t \in S$:
    $$\p{X_t = x_t \mid X_{t-1} = x_{t-1}, \ldots, X_0 = x_0} = \p{X_t = x_t \mid X_{t-1} = x_{t-1}}$$
    which implies
    $$\p{X_t = x_t} = \sum_{x_{t-1} \in S} \p{X_t = x_t \mid X_{t-1} = x_{t-1}} \p{X_{t-1} = x_{t-1}}$$
    For a random walk of some countable set of webpages $\Omega$, this property implies that 
    the probability that the web surfer lands on a certain webpage after clicking on $t$ links is solely dependent on
    which webpage the web surfer was on before clicking the $t$-th link.
\end{definition}

\begin{definition}
    Let a random walk of some finite set of $n$ webpages $\Omega$ be a Markov process on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$
    be described by $\{X_t : \Omega \to S\}_{t \in T}$, where $S = \{1, 2, \ldots, n\} = [n]$ and $T = \{0, 1, 2, 3, \ldots\} = \N$.
    
    Then, for each $t \in T$, let $\vec{v}_t$ be a $n \times 1$ vector
    % $\begin{bmatrix} v_1 \\ v_2 \\ \vdots \\ v_n \end{bmatrix}$,
    where $v_i = \p{X_t = i}$ for $i \in S$.
    Observe that $\vec{v}_t$ corresponds to the probability distribution of which webpage the web surfer is on after clicking $t$ links.
    We say that $\vec{v}_t$ is a probablistic vector describing the state at step $t$.

    Furthermore, let $P$ be a $n \times n$ matrix
    %$\begin{bmatrix}
    %    p_{11} & \cdots & p_{1n} \\
    %    \vdots & \ddots & \vdots \\
    %    p_{n1} & \cdots & p_{nn} \\
    %\end{bmatrix}$,
    where $p_{ij} = \p{X_t = i \mid X_{t-1} = j}$ for $i, j \in S$ and any arbitrary $t \in T$. Observe that by Definition 3, $\vec{v}_t = P \vec{v}_{t-1}$ for all $t \in T$.
    So multiplying by $P$ corresponds to the web surfer clicking a link.
    Thus we say that $P$ is the Markov transition matrix.
\end{definition}

\begin{definition}
    Let $A$ be an adjacency matrix that represents a directed graph consisted of some finite set of $n$ webpages enumerated 1 to $n$,
    with the $k$-th webpage represented as node $k$, and links between two webpages as edges,
    such that $A_{ij} = 1$ if and only if there exists an edge from node $j$ to node $i$, i.e. the $j$-th webpage has a link to the $i$-th webpage, and $A_{ij} = 0$ otherwise.
\end{definition}

\begin{definition}
    Let $L(j)$ denote the number of outgoing links from the $j$-th webpage.
    Given a $n \times n$ adjacency matrix $A$ representing a directed graph of $n$ webpages, 
    we have $L(j) = \sum_{i = 1}^n A_{ij}$.
    If there exists a node $k$ such that $L(k) = 0$, i.e. the $k$-th webpage has no outgoing links to other webpages, we say that node $k$ is a dangling node.
\end{definition}

\begin{definition}
    Let the Markov transition matrix $P$ of a random walk of some finite set of $n$ webpages be constructed as follows:
    given a $n \times n$ adjacency matrix $A$ representing a directed graph of the $n$ webpages,
    for each entry $p_{ij}$ in $P$, $i,j \in [n]$,
    $$ p_{ij} = \begin{cases}
        \dfrac{1}{L(j)} & \text{ if } A_{ij} = 1 \\
        \dfrac{1}{n} & \text{ if } A_{ij} = 0 \text{ and } L(j) = 0 \\
        0 & \text{ otherwise }
    \end{cases}$$
    Thus 
\end{definition}

\begin{definition}
    A matrix is said to be column-stochastic if and only if each of its entries are between 0 and 1 inclusive, and each of its columns sum up to 1.
    Similarly, a vector is said to be a stochastic or probablistic vector if and only if its components are between 0 and 1 inclusive and sum up to 1.
    Note that by definition, the Markov transition matrix $P$ is column-stochastic, and each $\vec{v}_t$ is a probablistic vector.
\end{definition}

\begin{definition}
    An Markov transition matrix is said to be irreducible if ...
    A positive Markov transition matrix with all positive entries is necessarily irreducible
    Note that the Markov transition matrix $P$ as constructed above is not necessarily irreducible.
\end{definition}

\begin{definition}
    Let $M$ denote the ``PageRank Matrix'' or ``Google Matrix'' defined by Page and Brin.
    Define 

    where $\vec{1}_{m \times n}$ denotes ...
    and $d$ denotes the ``damping factor'', representing the probability of a web surfer randomly traveling from one page to any other page. $0 \leq d < 1$.
\end{definition}

\begin{definition}
    Let $\lambda$ denote an eigenvalue of the square matrix $A$ and let $\vec{v}$ denote its corresponding eigenvector, such that $A \vec{v} = \lambda \vec{v}$.
    A probablistic eigenvector is...
\end{definition}

\section{Main Ideas}
Let $x$ be the PageRank vector, where the $i^{\textnormal{th}}$ entry of $x$ is the ranking of the $i^{\textnormal{th}}$ webpage.
We initialize $x$ as a vector with all entries equal to $\frac{1}{n}$ and then iteratively perform $t$ random walks through $n$ webpages.
Since $P$ is the Markov transition matrix, the product of $P^t$ and $x$ represents the ranks of each page after $t$ traversals.
We note that after infinitely many random walks, the probability that the web surfer is on a given page is proportional to the number of in-bound links to that page.
Therefore, we want the PageRank vector to be $\lim\limits_{t\to \infty}{A^tx}$, which is the stationary distribution of the Markov process.

In order to prove the existance of the PageRank vector, we first rely on the Perron-Frobenius Theorem.
\begin{theorem}
    (Perron-Frobenius Theorem) Consider a $n\times n$ positive column-stochastic matrix $M$. Let $\lambda_1, \lambda_2, \cdots, \lambda_n$ be the eigenvalues of $M$ sorted in decreasing order of magnitude. Then 1 is an eigenvalue with multiplicity 1 such that $\lambda_1=1$ and there exists a corresponding probabilistic eigenvector.
\end{theorem}

Since the PageRank matrix $M$ is a postitive column-stochastic matrix by construction, by Perron-Frobenius theorem we know that there exists a probabilistic eigenvector $\vec{v}^*$ corresponding to the eigenvalue 1.

In order to show that the PageRank vector is equal to $\vec{v}^*$, we apply the next theorem that exploits the fact that all of the eigenvectors $\lambda_i$ for $1<i\leq n$ of $M$ are less than $1$.
\begin{theorem}
    (Power Method Convergence Theorem) Let $M$ be a $n\times n$ positive column-stochastic matrix, with $\vec{v}^*$ denosting the probabilstic vector corresponding to 1. Let $x$ be the column vector with all entries equal to $\frac{1}{n}$. Then the sequence $x, Mx, \cdots, M^kx$ as $k\to \infty$ converges to $\vec{v}^*$.    
\end{theorem}

Thus, the final PageRank vector is equal to the probabilistic eigenvector corresponding to the eigenvalue of 1.

We note that both of these theorems only apply to positive column-stochastic matrices.
We shall now examine two special cases that illuminate why the matrix must be positive.
Consider a graph that contains a dangling node. This means that the matrix $A$, and therefore $P$, would contain a column full of 0s, which means that $P^tx$ would converge to $\vec{0}$, regardless of the other links recorded in $P$.
If a matrix is reducible, we see that $P^tx$ would also have some webpages that have in-bound links converge to 0.
In order to prevent the first convergence error from occuring, we define $P$ such that a dangling node has equal probability of transitioning to any random webpage, synonymous to how a web surfer can type a hyperlink directly to a webpage.
In order to handle reducible matrices, we convert $P$ into a positive matrix by considering the probabilities of the web surfer ``jumping" to a random page for every page. This yields the PageRank matrix $M$, which weighs the probability of following a link and randomly ``jumping" to create a positive column-stochastic matrix.

\begin{lstlisting}[language=python, basicstyle={\small\ttfamily}, numbers=left]
def build_prob_matrix(adj_list):
# number of nodes
n = len(adj_list)
P = np.zeros((n,n), dtype=float)
for (j, connected_nodes) in enumerate(adj_list):
    if connected_nodes: # non-empty
        P[connected_nodes, j] = 1.0 / len(connected_nodes)
    else: # dangling node
        P[:, j] = 1.0 / n
return P
\end{lstlisting}

% \section{Results}

\section{Conclusion}


\begin{thebibliography}{9}
\bibitem{}

\end{thebibliography}
\end{document}


