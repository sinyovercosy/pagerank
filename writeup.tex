% Homework LaTeX Template
% original version by Sayan Chaudhry (sayanc)
% modified by Tan Yan (tanyan)

\documentclass[11pt]{article}

\title{PageRank}
\author{Tan Yan \and Daniel Bae}

% Useful Packages
\usepackage{amsmath}  % Packages to make typesetting math
\usepackage{amssymb}  %   stuff and symbols easy by the
\usepackage{amsthm}   %   American Mathematical Society
\usepackage{textcomp} % Package for some special symbols
\usepackage{fancyhdr} % Package to make pretty headers
\usepackage[latin1]{inputenc}
\usepackage{enumerate}
\usepackage[hang,flushmargin]{footmisc}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{hyperref} % Package to add image support
\hypersetup{          
    colorlinks=true,  
    linkcolor=red,    
    filecolor=magenta,      
    urlcolor=cyan,
}
\usepackage{graphicx} % Package to add image support
\graphicspath{        
    {assets/}         % Upload all images in assets/
}

% Theorem Styles
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{notation}[theorem]{Notation}

% Page Setup
\oddsidemargin0cm
\topmargin-2cm
\textwidth16.5cm
\textheight23.5cm
\setlength{\parindent}{0pt}
\setlength{\parskip}{5pt plus 1pt}
\setlength{\headsep}{1.7em}
\setlength{\headheight}{30pt}
\renewcommand{\baselinestretch}{1.25}

% Math Commands :: General
\newcommand{\micdrop}{\hfill \qedsymbol}
\newcommand{\abs}[1]{\left| #1\right|}
\newcommand{\floor}[1]{\left\lfloor#1\right\rfloor}
\newcommand{\Lap}[1]{\mathcal{L}\left\{#1\right\}}

% Math Commands :: Proofs 
\newcommand{\wts}{\text{We want to show that }}
\newcommand{\st}{\text{ such that }}
\newcommand{\afsoc}{\text{Assume for the sake of contradiction that }}
\newcommand{\Wlog}{\text{Without loss of generality, assume that }}

% Math Commands :: Sets
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\U}{\mathcal{U}}
\renewcommand{\emptyset}{\varnothing}

% Math Commands :: Induction
\newcommand{\rhs}{\texttt{RHS}}
\newcommand{\lhs}{\texttt{LHS}}
\newcommand{\bc}{\texttt{Base Case}}
\newcommand{\bcs}{\texttt{Base Cases}}
\newcommand{\ih}{\texttt{Induction Hypothesis}}
\newcommand{\is}{\texttt{Induction Step}}

% Math Commands :: Probability
\newcommand{\p}[1]{\mathbb{P}\left\{#1\right\}}
\newcommand{\e}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\var}[1]{\mathrm{Var}\left[#1\right]}

% Math Commands :: Linear Algebra
\DeclareMathOperator{\Diam}{diam}
\newcommand{\diam}[1]{\Diam\left(#1\right)}
\DeclareMathOperator{\nul}{Null}
\newcommand{\Null}[1]{\nul\left(#1\right)}
\renewcommand{\vec}[1]{\mathbf{#1}}
\newcommand{\threevec}[3]{\left[\begin{array}{r} #1 \\ #2 \\ #3\end{array}\right]}
\newcommand{\fourvec}[4]{\left[\begin{array}{r} #1 \\ #2 \\ #3\\#4\end{array}\right]}
\DeclareMathOperator{\Rank}{rank}
\newcommand{\rank}[1]{\Rank\left(#1\right)}
\DeclareMathOperator{\Sp}{Span}
\renewcommand{\sp}[1]{\Sp\left\{#1\right\}}
\DeclareMathOperator{\col}{Col}
\newcommand{\Col}[1]{\col\left(#1\right)}

\begin{document}
\maketitle

\section{Introduction}
The early search engines of the Internet used text-based ranking systems that assigned relevance to webpages based on the highest number of keywords present. 
Although this approach intuitively makes sense, it often ranked webpages with a high volume of keywords and no other content as relevant results to a search, which is not desired behavior.
A better approach to generating webpage rankings is to examine the links to that page.
Under this schema, a page is highly ranked if it has many links or a highly-ranked page directed to it, encapsulating both popularity and authority into a ranking.

\section{Definitions}
\begin{definition}
    A stochastic matrix, probability matrix, or Markov transition matrix is ...
    A column-stochastic or left-stochastic matrix is ...
\end{definition}

\begin{definition}
    A positive matrix is a matrix with all positive entries.
\end{definition}

\begin{definition}
    An Markov transition matrix is said to be irreducible if ...
    A positive stochastic matrix is necessarily irreducible.
\end{definition}

\begin{definition}
    Let $G$ be a directed graph of the webpages, with each webpage as a node and links between the webpages as edges ... such that $G_{ij} = 1$ if ...
    also let $n$ denote the number of nodes ...
    let $L(i)$ denote the number of outgoing edges ...
\end{definition}

\begin{definition}
    A dangling node is a node that has no outgoing edges
\end{definition}

\begin{definition}
    Let $P$ be the probablistic matrix 
\end{definition}

\begin{definition}
    Let $d$ denote the ``damping factor'' ... representing the probability of a web surfer randomly traveling from one page to any other page. $0 \leq d < 1$.
\end{definition}

\begin{definition}
    Let $\vec{1}$ 
\end{definition}

\begin{definition}
    Let $M$ denote the ``PageRank Matrix'' or ``Google Matrix'' defined by Page and Brin.
    Define 
\end{definition}

\begin{definition}
    Let $\lambda$ denote an eigenvalue of the square matrix $A$ and let $\vec{v}$ denote its corresponding eigenvector, such that $A \vec{v} = \lambda \vec{v}$.
    A probablistic eigenvector is...
\end{definition}

\section{Main Ideas}
We can design a Markov chain for the network of webpages and links, which is represented by $G$.
Given $G$, we can construct $P$ such that $P_{ij}$ is the probability that a surfer on page $i$ clicks on a link to page $j$. 
If we assume that each page has an equal probability of being selected, the column vector $P_i$ contains $L(i)$ entries with value $\frac{1}{L(i)}$ when the edge from $i$ to $j$ exists in $G$ and 0's in the other entries.

Let $x$ be the PageRank vector, where the $i^{\textnormal{th}}$ entry of $x$ is the ranking of the webpage.
We initialize $x$ as a vector with all entries as $\frac{1}{n}$ and then iteratively traverse through $G$, updating the ranks after each traversal.
Since $P$ is the Markov transition matrix of $G$, the product of $P^m$ and $x$ represents the ranks of each page after $m$ traversals though $G$.
We note that after infinitely many random walks, the probability that the web surfer is on a given page is proportional to the number of in-bound links to the 
Therefore, the PageRank vector is the stationary distribution of $G$.

However, we note that there are two special cases that can lead to $x$ not converging properly.
Consider the case when a random web surfer reaches a webpage by directly typing in its hyperlink. This would be like ``jumping" from a node $i$ in $G$ to any other node $j$ in $G$, regardless of if the edge $(i,j)$ exists in $G$.
If we assume that each webpage is equally likely to be ``jumped" to by the random web surfer, we obtain a new $n\times n$ probabilstic matrix, which we will denote as $B$, where all of the elements are $\frac{1}{n}$ to represent the probability of jumping from a webpage to another webpage. Since we know $d$ is the probability that the surfer will ``jump",  we can now define the PageRank matrix $M$ to be $(1-d)P + dB$.

In order to prove the existance of the PageRank vector, we first rely on the Perron-Frobenius Theorem.
\begin{theorem}
    (Perron-Frobenius Theorem) Consider a $n\times n$ positive column-stochastic matrix $M$. Let $\lambda_1, \lambda_2, \cdots, \lambda_n$ be the eigenvalues of $M$ sorted in decreasing order of magnitude. Then 1 is an eigenvalue with multiplicity 1 such that $\lambda_1=1$ and there exists a corresponding probabilistic eigenvector.
\end{theorem}

Since the PageRank matrix $M$ is constructed to be a postitive column-stochastic matrix, by Perron-Frobenius theorem we know that there exists a probabilistic eigenvector $\vec{v}^*$ corresponding to the eigenvalue 1.

The next theorem uses the fact that all of the eigenvectors $\lambda_i$ for $1<i\leq n$ of $M$ are less than $1$ to show that the PageRank vector is equal to $\vec{v}^*$.
\begin{theorem}
    (Power Method Convergence Theorem) Let $M$ be a $n\times n$ positive column-stochastic matrix, with $\vec{v}^*$ denosting the probabilstic vector corresponding to 1. Let $z$ be the column vector with all entries equal to $\frac{1}{n}$. Then the sequence $z, Mz, \cdots, M^kz$ as $k\to \infty$ converges to $\vec{v}^*$.    
\end{theorem}

\begin{lstlisting}[language=python, basicstyle={\small\ttfamily}, numbers=left]
def build_prob_matrix(adj_list):
# number of nodes
n = len(adj_list)
P = np.zeros((n,n), dtype=float)
for (j, connected_nodes) in enumerate(adj_list):
    if connected_nodes: # non-empty
        P[connected_nodes, j] = 1.0 / len(connected_nodes)
    else: # dangling node
        P[:, j] = 1.0 / n
return P
\end{lstlisting}

% \section{Results}

\section{Conclusion}

\begin{thebibliography}{9}
\bibitem{}

\end{thebibliography}
\end{document}


